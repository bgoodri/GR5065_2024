---
title: "Introduction to Hierarchical Models"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

## Obligatory Disclosure

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```
-   Ben is an employee of Columbia University, which has received several research grants to develop Stan

-   Ben is also a manager of GG Statistics LLC, which uses Stan

-   According to Columbia University [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who has any equity stake in, a title (such as officer or director) with, or is expected to earn at least $\$5,000.00$ per year from a private company is required to disclose that

## Don't Call It a Comeback ([PredictIt](https://www.predictit.org/markets/detail/7456/Who-will-win-the-2024-US-presidential-election))

```{r}
#| echo: false
#| message: false
library(dplyr)
library(ggplot2)
predictit <- readr::read_csv("predictit.csv", show_col_types = FALSE) |> 
  filter(ContractName == "Trump" | ContractName == "Biden") |> 
  mutate(across(ends_with("Price"), ~as.numeric(gsub("$", "", .x, fixed = TRUE))),
         Date = as.Date(gsub(" .*$", "", Date), format = "%m/%d/%Y"))
ggplot(predictit) +
  geom_line(aes(x = Date, y = CloseSharePrice, color = ContractName)) +
  ylim(0.35, 0.55) +
  scale_color_manual(values = c("blue", "red")) +
  theme(legend.position = "top") +
  labs(x = "Date", y = "Contract Closing Price in Cents")
```

## What Are Hierarchical Models

-   In Bayesian terms, a hierarchical model is nothing more than a model where the prior for some parameter depends on another

-   We have already seen several examples of hierarchical structures:

    -   Bowling: $x_2$ depends on $n = 10 - x_1$
    -   HW3: Veto overrides depends on the number of regular vetoes
    -   Linear models: $\sigma \thicksim \mathcal{E}\left(r\right)$ and $\forall n: \epsilon_n \thicksim \mathcal{N}\left(0,\sigma\right)$

-   It is just another application of the rules of probability: $f\left(\boldsymbol{\theta}\right) = \int f\left(\boldsymbol{\theta}, \boldsymbol{\phi}\right) d\phi_1 \dots d\phi_K = \int f\left(\boldsymbol{\theta} \mid \boldsymbol{\phi}\right) f\left(\boldsymbol{\phi}\right) d\phi_1 \dots d\phi_K$

## Splines

-   "Linear" models are linear functions of the coefficients, but you can always include nonlinear functions of predictors

    -   Interaction terms where three columns of $X$ are generated by two variables and their product
    -   Polynomials where $K$ columns of $X$ are generated by one variable raised to $K$ different powers
    -   Dummy variables that are generated by a factor with $K + 1$ levels

-   Splines use "basis functions", which are much better than polynomials if you want $\eta_n$ to depend on a predictor in a continuous but nonlinear way

## Okun's Law Data

```{r}
source(file.path("..", "Week07", "macroeconomic_data.R"))
G <- mgcv::gam(GDO ~ s(x), data = data, fit = FALSE)
X <- G$X
X <- X[ , -1] # these columns each have a mean of zero
colnames(X) <- paste0("X_", 1:ncol(X))
X <- as_tibble(X)
dim(X) # 9 basis functions of change in unemployment
```

Okun never said the relationship was theoretically linear, although he estimated models that assumed so, which is extremely common in the social sciences.

## Model with a Spline

::: columns
::: {.column width="46%"}
Math Notation $\begin{eqnarray*} \forall n: y_n & \equiv & \mu_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \mu_{n} & \equiv & \alpha + \\ && \sum_{k = 1}^K \beta_{k} b_k\left(x_{nk}\right) \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \forall k: \beta_{k} & \thicksim & \mathcal{N}\left(0, \sigma_\beta\right) \\ \sigma_\beta & \thicksim & \mathcal{E}\left(r_\beta\right) \end{eqnarray*}$
:::

::: {.column width="54%"}
::: fragment
Code to Draw

```{r}
R <- 100
priors <- tibble (
  alpha = rnorm(R, 2.5, 0.5),
  sigma_beta = rexp(R, 2),
  sigma_epsilon = rexp(R, 0.2),
  beta = matrix(nrow = R, data = 
    rnorm(R * ncol(X), 
          sd = sigma_beta))
)
predictions <- 
  cross_join(X, priors) |> 
  transmute(mu = alpha + rowSums(
              pick(starts_with("X_")) 
                   * beta),
            epsilon = 
              rnorm(n(), sd =
                    sigma_epsilon),
            y = mu + epsilon)
```
:::
:::
:::

## Posterior Distribution

```{r, Okun}
#| cache: true
#| results: hide
post <- rstanarm::stan_gamm4(GDO ~ s(x), data = data, seed = 12345,
                             prior_intercept = rstanarm::normal(2.5, 0.5),
                             prior_aux = rstanarm::exponential(0.2))
```

```{r}
post
```

## Posterior Plot

```{r}
rstanarm::plot_nonlinear(post)
```

## Restricted Cubic Splines (RCS)

Often it is reasonable to assume a nonlinear function becomes linear through the most extreme data points on either side, which can be accomplished by restricting the derivatives.

```{r}
rstanarm::stan_lm(GDO ~ rms::rcs(x), data = data, seed = 12345,
                  prior_intercept = rstanarm::normal(2.5, 0.5),
                  prior = rstanarm::R2(0.75), refresh = 0, 
                  adapt_delta = 0.999) # to prevent divergences
```

## Gaussian Process (GP) Approximation

-   A Gaussian Process is a prior over smooth functions

-   This requires $N^3$ floating-point operations to evaluate the log-likelihood once, so it can be prohibitive for some datasets that you will encounter in the social sciences

-   The brms package includes a GP approximation that is faster

```{r}
library(brms)
options(mc.cores = parallel::detectCores())
get_prior(GDO ~ gp(x, k = 5, c = 5 / 4), data = data)
```

## GP Approximation Example

```{r, GP}
#| cache: true
#| results: hide
post <- brm(GDO ~ gp(x, k = 5, c = 5 / 4), data = data, prior = 
              prior(normal(2.5, 0.5), class = "Intercept") + 
              prior(exponential(1), class = "sdgp") + 
              prior(exponential(1), class = "sigma"),
            control = list(adapt_delta = 0.9))
```

```{r}
post
```

## Conditional Effects in GPs

```{r}
plot(conditional_effects(post))
```

## Cluster vs. Stratified Sampling

-   For cluster random sampling, you

    -   Sample $J$ large units from their population
    -   Sample $N_j$ small units from the $j$-th large unit

-   For stratified random sampling, you

    -   Divide the population of large units into $J$ mutually exclusive and exhaustive groups that are not RVs
    -   Sample $N_j$ small units from the $j$-th large unit

## Models with Group-Specific Intercepts {.smaller}

-   Let $\alpha$ be the common intercept and $\boldsymbol{\beta}$ be the common coefficients while $a_j$ is the deviation from the common intercept in the $j$-th group. Write a model as: $$y_{ij} = \overbrace{\underbrace{\alpha + \sum_{k = 1}^K \beta_k x_{ik}}_{\mbox{Frequentist }
    \boldsymbol{\mu} \mid \mathbf{x}}+a_j}^{\mbox{Bayesian } \boldsymbol{\mu} \mid \mathbf{x},j} +
    \epsilon_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik}+\underbrace{a_j + 
    \overbrace{\epsilon_{ij}}^{\mbox{Bayesian error}}}_{\mbox{Frequentist error}}$$
-   The same holds in GLMs where $\eta_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik} + a_j$ or $\eta_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik}$ depending on if you are Bayesian or Frequentist

## Models with Group-Specific Slopes {.smaller}

-   Let $\alpha$ be the common intercept and $\boldsymbol{\beta}$ be the common coefficients while $a_j$ is the deviation from the common intercept in the $j$-th group and $\mathbf{b}_j$ is the deviation from the common coefficients. Write the model as: $$y_{ij} = \overbrace{\underbrace{\alpha + \sum_{k = 1}^K \beta_k x_{ik}}_{\mbox{Frequentist }
    \boldsymbol{\mu} \mid \mathbf{x}} + a_j + \sum_{k = 1}^K b_{jk} x_{ik}}^{\mbox{Bayesian } \boldsymbol{\mu} \mid \mathbf{x},j} + \epsilon_{ij} = \\ \alpha + \sum_{k = 1}^K \beta_k x_{ik}+\underbrace{a_j + \sum_{k = 1}^K b_{jk} x_{ik} + \overbrace{\epsilon_{ij}}^{\mbox{Bayesian error}}}_{\mbox{Frequentist error}}$$
-   And similarly for GLMs that utilize an inverse link function

## [Frequentist Estimation of MLMs](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)

-   Frequentists assume that $a_j$ and $b_j$ deviate from the common parameters according to a (multivariate) normal distribution, whose (co)variances are common parameters
-   To Frequentists, $a_j$ and $b_j$ are not parameters because parameters must remain fixed in repeated sampling of observations from some population
-   Since $a_j$ and $b_j$ are not parameters, they can't be "estimated" only "predicted"
-   Since $a_j$ and $b_j$ aren't estimated, they must be integrated out of the likelihood function, leaving an integrated likelihood function of the common parameters

## Bivariate Normal over $S = \mathbb{R}^2$

$f\left(x,y\mid \mu_X,\mu_Y,\sigma_X,\sigma_Y,\rho\right) =\\ \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}e^{-\frac{1}{2\left(1-\rho^2\right)} \left(\left(\frac{x - \mu_X}{\sigma_X}\right)^2 + \left(\frac{y - \mu_Y}{\sigma_Y}\right)^2 - 2\rho\frac{x - \mu_X}{\sigma_X}\frac{y - \mu_Y}{\sigma_Y}\right)} = \\ \frac{1}{\sigma_X\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu_X}{\sigma_X}\right)^2} \times \frac{1}{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}\sqrt{2\pi}}e^{-\frac{1}{2} \left(\frac{y - \left(\color{red}{\mu_Y + \frac{\sigma_Y}{\sigma_X}\rho\left(x-\mu_x\right)}\right)} {\color{blue}{\sigma_Y\sqrt{1-\rho^2}}}\right)^2}$ where the first term is a marginal normal PDF for $X$ and the second is a conditional normal PDF for $Y \mid X = x$ with new parameters $\color{red}{\mu = \mu_Y + \frac{\sigma_Y}{\sigma_X}\rho\left(x-\mu_X\right)}$ & $\color{blue}{\sigma = \sigma_Y\sqrt{1-\rho^2}}$

## Multilevel Data-Generating Processes {.smaller}

::: columns
::: {.column width="50%"}
Bayesian $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \eta_{n} & \equiv & \alpha + a_j + \left(\beta + b_j\right) x_{n} \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \beta & \thicksim & \mathcal{N}\left(m_1, s_1\right) \\ \forall j: b_j & \thicksim & \mathcal{N}\left(\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \thicksim & \mathcal{E}\left(r\right) \\ \rho & \thicksim & \mathcal{U}\left(-1,1\right) \end{eqnarray*}$
:::

::: {.column width="50%"}
::: fragment
Frequentist $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(a_j + b_j x_n, \sigma_\epsilon\right) \\ \sigma_\epsilon & \text{is} & \text{given} \\ \forall n: \eta_{n} & \equiv & \alpha + \beta + x_{n} \\ \alpha & \text{is} & \text{given} \\ \beta & \text{is} & \text{given} \\ \forall j: b_j & \thicksim & \mathcal{N}\left(\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \text{are} & \text{given} \\ \rho & \text{is} & \text{given} \end{eqnarray*}$
:::
:::
:::

## Table 2 from the lme4 [Vignette](https://www.jstatsoft.org/article/view/v067i01/0)

![](lme4Syntax.png)

## Hierarchical Measurement Models

```{r}
#| message: false
rstanarm::stan_glmer(y ~ 1 + (1 | quarter_startdate), 
                     family = gaussian,
                     data = tidyr::pivot_longer(data, c(GDI, GDP), 
                                                values_to = "y"),
                     subset = quarter_startdate < "2020-01-01",
                     prior_intercept = rstanarm::normal(2.5, 0.5))
```

## First Quarter of 2024

-   At 8:30 AM on April 25, the US government will [release](https://www.bea.gov/) its first estimate of GDP growth for the first quarter of 2024

-   What do you think $\mu_t$ is for the first quarter of 2024?

-   A WSJ [survey](https://s.wsj.net/public/resources/documents/wsjecon0424.xlsx) of 68 economists had a mean of 2.18 and a standard deviation of 0.38, which is considerably higher than those same economists predicted in January

> [Sam Goldfarb](https://www.wsj.com/podcasts/whats-news/economists-expectations-for-the-us-economy/89ab411c-5e23-4162-b0ac-97410c221708): You can definitely get cynical about these things and say, "Hey, these economists aren't a hundred percent correct all the time. Why should we care?" They're not maybe as wrong as you might think.

## Hierarchical Models in Psychology

-   In political science and economics, the "big" units are often countries or sub-national political areas like states and the "small" units are people
-   In [psychology](https://arxiv.org/pdf/1506.04967.pdf), the "big" units are often people and the "small" units are questions or outcomes on repeated tasks
-   Hierarchical model syntax is like

```{r, eval = FALSE}
y ~ x + (x | person) + (1 | question)
```

-   Question of interest is how to predict `y` for a new "big" unit (person), as opposed to predicting how well an old "big" unit will answer a new "small" unit (question)

## Hierarchical Default Priors

```{r, message = FALSE}
# from http://www.tqmp.org/RegularArticles/vol14-2/p099/p099.pdf
dat <- readr::read_csv("https://osf.io/5cg32/download")
```

```{r}
get_prior(valence ~ arousal + (1 + arousal | PID), data = dat) |>  
  as_tibble() |> 
  select(prior, class, coef) 
```

## Hierarchical PPD {.smaller}

::: columns
::: {.column width="50%"}
Math Notation $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \eta_{n} & \equiv & \alpha + a_j + \left(\beta + b_j\right) x_{n} \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \beta & \thicksim & \mathcal{N}\left(m_1, s_1\right) \\ \forall j: b_j & \thicksim & \mathcal{N}\left(\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \thicksim & \mathcal{E}\left(r\right) \\ \rho & \thicksim & \mathcal{U}\left(-1,1\right) \end{eqnarray*}$
:::

::: {.column width="50%"}
::: fragment
Code to Draw

```{r}
R <- 1000
priors <- tibble (
  rho = runif(R, min = -1, max = 1),
  sigma_a = rexp(R, 0.5),
  sigma_b = rexp(R, 0.5),
  b_a = sigma_b / sigma_a * rho,
  s = sqrt(1 - rho^2) * sigma_b,
  beta = rnorm(R, 0, 1),
  alpha = rnorm(R, 50, 20),
  sigma = rexp(R, 0.1)
)
predictions <- cross_join(dat, priors) |> 
  group_by(PID) |> # critical!
  transmute(a = rnorm(n(), sd = sigma_a),
            b = rnorm(n(), mean = b_a * a,
                      sd = s),
            eta = alpha + a + 
              (beta + b) * arousal,
            epsilon = rnorm(n(), sd = sigma),
            y = eta + epsilon) |> 
  ungroup()
```
:::
:::
:::

## Hierarchical Example

```{r, psych}
#| cache: true
#| results: hide
post <-  brm(valence ~ arousal + (1 + arousal | PID), data = dat,
             prior = prior(normal(50, 20), class = "Intercept") +
               prior(normal(0, 1), class = "b") +
               prior(exponential(0.1), class = "sigma") +
               prior(exponential(0.5), class = "sd"),
             control = list(adapt_delta = 0.97))
```

```{r}
post
```

## PSISLOOCV Estimator of ELPD

```{r}
loo(post) # there were 46 nominal unknowns
```

## Posterior Predictive Checks

```{r}
pp_check(post, type = "ribbon_grouped", x = "arousal", group = "PID")
```

## Posterior Prediction

```{r}
nd <- filter(dat, PID == 1) |> 
  mutate(PID = 0) # now predict for person 0 who did not exist before
y_0 <- posterior_predict(post, newdata = nd, allow_new_levels = TRUE)
```

How is that even possible? For each of the $R$ posterior draws,

1.  Draw $a_0$ and $b_0$ from a bivariate normal given $\sigma_a$, $\sigma_b$, $\rho$
2.  Form $\boldsymbol{\mu} \equiv \alpha + a_0 + \left(\beta + b_0\right) \mathbf{x}$
3.  Draw each $\epsilon$ from a normal distribution with mean zero and standard deviation $\sigma$
4.  Form the prediction vector $\mathbf{y} = \boldsymbol{\mu} + \boldsymbol{\epsilon}$

This process yields much better predictions for new big units than supervised learning of Frequentist prediction methods
